{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoria\n",
    "\n",
    "La memoria en los modelos de lenguaje (LLMs) permite que estos recuerden información a lo largo de una conversación, simulando la capacidad de \"memoria a largo plazo\". Esto es útil para mantener un contexto continuo en las interacciones.\n",
    "¿Cómo funciona la memoria?\n",
    "\n",
    "La memoria almacena un historial de las interacciones entre el usuario y el modelo, enviándolo como parte del prompt en cada nueva consulta. Sin embargo, debido a la ventana de contexto limitada de los LLMs, se deben gestionar las implementaciones de memoria para optimizar el rendimiento y garantizar respuestas coherentes.\n",
    "Tipos de memoria en LangChain\n",
    "\n",
    "- Conversation Buffer: Guarda el historial completo de mensajes.\n",
    "- Conversation Buffer Window: Retiene solo los mensajes más recientes.\n",
    "- Conversation Summary: Resume el historial para reducir el contexto.\n",
    "- Knowledge Graph Memory: Organiza el conocimiento en forma de grafo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conversation Buffer\n",
    "\n",
    "Esta es la implementación más básica de memoria. Almacena cada mensaje de entrada (prompt) y salida (respuesta) como pares \"Human\" y \"AI\". Cada vez que se envía una nueva consulta, todo el historial se adjunta al prompt.\n",
    "Ventajas\n",
    "\n",
    "- Permite al modelo recordar toda la conversación.\n",
    "- Es fácil de implementar.\n",
    "\n",
    "Desventajas\n",
    "\n",
    "- Consume rápidamente la ventana de contexto del modelo si la conversación es larga.\n",
    "- Puede ser ineficiente para interacciones extensas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "api_key = config.OPENAI_API_KEY\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Configuración del modelo y la memoria\n",
    "llm = ChatOpenAI(openai_api_key=api_key)  # Modelo de lenguaje\n",
    "memoria = ConversationBufferMemory()  # Inicializamos la memoria\n",
    "chatbot = ConversationChain(llm=llm, memory=memoria, verbose=True)  # Creamos el chatbot\n",
    "\n",
    "# Ejemplo de interacción\n",
    "chatbot.predict(input=\"¿Cómo me llamo?\")  # El modelo no lo sabrá en esta primera interacción\n",
    "\n",
    "# Visualización del historial de memoria\n",
    "memoria.chat_memory.messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conversation Buffer Window Memory\n",
    "\n",
    "La Conversation Buffer Window Memory es similar al modelo de memoria anterior (Conversation Buffer), pero con una diferencia clave: en lugar de almacenar todo el historial de interacciones, se mantiene solo una \"ventana\" de mensajes recientes. Esto permite reducir el consumo de contexto, enfocándose en las interacciones más recientes.\n",
    "Ventajas\n",
    "\n",
    "- Es más eficiente que la memoria de buffer completa en conversaciones largas.\n",
    "- Evita que el contexto relevante se pierda entre interacciones pasadas.\n",
    "\n",
    "Desventajas\n",
    "\n",
    "- Si la ventana es muy pequeña, el modelo puede perder información importante de interacciones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Configuración del modelo y la memoria\n",
    "llm = ChatOpenAI()  # Modelo de lenguaje\n",
    "memoria = ConversationBufferWindowMemory(k=2)  # La ventana se limita a los últimos 2 mensajes\n",
    "chatbot = ConversationChain(llm=llm, memory=memoria, verbose=True)  # Crear el chatbot\n",
    "\n",
    "# Ejemplo de interacción\n",
    "chatbot.predict(input=\"¿Cómo me llamo?\")  # El modelo no lo sabrá aún\n",
    "\n",
    "# Visualización del historial en la ventana\n",
    "memoria.chat_memory.messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conversation Summary Memory\n",
    "\n",
    "La Conversation Summary Memory es una implementación de memoria que no guarda todo el historial de interacciones ni los mensajes recientes como en los casos anteriores, sino que almacena un resumen de la conversación. Esto reduce significativamente el tamaño del prompt enviado al modelo y es útil para mantener un contexto general sin saturar la ventana de contexto del LLM.\n",
    "Ventajas\n",
    "\n",
    "- Ideal para conversaciones largas, ya que evita prompts extensos.\n",
    "- Proporciona un contexto condensado de la interacción.\n",
    "- Permite que el modelo conserve información clave sin almacenar todos los detalles.\n",
    "\n",
    "Desventajas\n",
    "\n",
    "- La precisión depende de la calidad del resumen generado por el modelo.\n",
    "- Información específica podría perderse al ser condensada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Configuración del modelo y la memoria de resumen\n",
    "llm = ChatOpenAI(stop=[\"\\nHuman\"])  # Configuración del modelo\n",
    "memoria = ConversationSummaryMemory(llm=llm)  # Configuración de memoria basada en resúmenes\n",
    "chatbot_resumen = ConversationChain(llm=llm, memory=memoria, verbose=True)  # Crear el chatbot\n",
    "\n",
    "# Ejemplo de interacción\n",
    "chatbot_resumen.predict(input=\"¿Qué te gusta a ti de la tecnología?\")  # Primera pregunta\n",
    "\n",
    "# Ver el contenido de la memoria\n",
    "memoria.chat_memory.messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conversation Knowledge Graph Memory\n",
    "\n",
    "La Conversation Knowledge Graph Memory almacena el contexto de las interacciones en forma de un grafo de conocimiento. Este enfoque permite identificar relaciones clave y entidades en la conversación, organizándolas de manera estructurada.\n",
    "Ventajas\n",
    "\n",
    "- Útil para extraer relaciones entre entidades (personas, objetos, conceptos) en la conversación.\n",
    "- Facilita un razonamiento más avanzado al organizar la información como triples (sujeto, predicado, objeto).\n",
    "- Ayuda a capturar conocimiento semántico.\n",
    "\n",
    "Desventajas\n",
    "\n",
    "- No es ideal si las interacciones no tienen entidades o relaciones claras.\n",
    "- Requiere procesamiento adicional para construir y mantener el grafo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Importar herramientas adicionales para visualizar grafos\n",
    "import networkx as nx\n",
    "\n",
    "# Configuración del modelo y la memoria basada en grafos\n",
    "llm = ChatOpenAI(stop=[\"\\nHuman\"])\n",
    "memoria = ConversationKGMemory(llm=llm)  # Crear la memoria basada en Knowledge Graph\n",
    "chatbot_kgm = ConversationChain(llm=llm, memory=memoria, verbose=True)  # Crear el chatbot\n",
    "\n",
    "# Ejemplo de interacción\n",
    "chatbot_kgm.predict(input=\"¿Qué estudió Santi?\")  # Primera interacción\n",
    "\n",
    "# Mostrar los triples extraídos del grafo\n",
    "print(chatbot_kgm.memory.kg.get_triples())\n",
    "\n",
    "# Mostrar el historial de la memoria\n",
    "memoria.chat_memory.messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando GPT-3 (Instruct models como Davinci, Ada, Curie o Babbage)\n",
    "\n",
    "Las implementaciones anteriores, basadas en GPT-3.5 Turbo, también pueden adaptarse fácilmente para usar modelos GPT-3 instructivos como Davinci, Ada, Curie o Babbage. Estos modelos están diseñados para instrucciones directas y ofrecen diferentes niveles de rendimiento y costos según la tarea.\n",
    "Diferencias clave al usar modelos instructivos:\n",
    "\n",
    "   1. Modelo configurado: Se utiliza la clase OpenAI en lugar de ChatOpenAI.\n",
    "2. Configuración más sencilla: Estos modelos no manejan mensajes en forma de pares de \"Human\" y \"AI\", sino que trabajan directamente con texto plano.\n",
    "3. Control de temperatura: El parámetro temperature permite ajustar la creatividad o precisión del modelo (valores más bajos son más deterministas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Configuración del modelo\n",
    "llm = OpenAI(temperature=0)  # Usando Davinci, Ada, Curie o Babbage\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()  # Usando memoria de buffer\n",
    ")\n",
    "\n",
    "# Ejemplo de interacción\n",
    "conversation.predict(input=\"¿Cómo me llamo?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
