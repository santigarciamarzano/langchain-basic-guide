{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modelos\n",
    "\n",
    "Los modelos son el núcleo de LangChain y se encargan de procesar texto. Puedes elegir entre diferentes modelos en función de tu necesidad:\n",
    "\n",
    "- OpenAI: Ideal para tareas generales como generación de texto o respuestas a preguntas.\n",
    "- LlamaCpp: Diseñado para implementaciones locales y entornos sin conexión, útil si necesitas un control completo sobre el modelo.\n",
    "\n",
    "¿Cuándo usarlo?\n",
    "\n",
    "- OpenAI: Cuando necesites alto rendimiento, fácil configuración y estés dispuesto a usar un servicio basado en la nube.\n",
    "- LlamaCpp: Si prefieres trabajar localmente y manejar modelos personalizados sin depender de servicios externos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp, OpenAI \n",
    "import config\n",
    "\n",
    "api_key = config.OPENAI_API_KEY  \n",
    "\n",
    "# Modelo basado en OpenAI\n",
    "llm_openai = OpenAI(model_name=\"text-davinci-003\", openai_api_key=api_key)\n",
    "respuesta_openai = llm_openai(\"Hola, ¿cómo estás?\")\n",
    "print(respuesta_openai)\n",
    "\n",
    "# Modelo local con LlamaCpp\n",
    "# llm_llama = LlamaCpp(model_path=\"./llamacpp/models/7B/ggml-model-q4_0.bin\")\n",
    "# respuesta_llama = llm_llama(\"Hola, ¿cómo estás?\")\n",
    "# print(respuesta_llama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modelos de Chat\n",
    "\n",
    "Los modelos de chat, como ChatGPT, están diseñados específicamente para interacción conversacional. Estos modelos procesan mensajes estructurados para emular un diálogo fluido.\n",
    "¿Por qué usar modelos de chat?\n",
    "- Se ajustan mejor a flujos de conversación en tiempo real.\n",
    "- Pueden manejar instrucciones específicas para cada rol (usuario, sistema, modelo).\n",
    "- Son ideales para asistentes virtuales o chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicación típica: Chatbots personalizados, sistemas de atención al cliente, aplicaciones que requieren respuestas contextuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chatgpt = ChatOpenAI(openai_api_key=api_key)\n",
    "\n",
    "# Enviando un mensaje al modelo\n",
    "respuesta = chatgpt([HumanMessage(content=\"Hola, ¿cómo estás?\")])\n",
    "print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prompts\n",
    "\n",
    "Los prompts son la base para interactuar con modelos de lenguaje. Son instrucciones o preguntas que guían al modelo sobre qué respuesta se espera.\n",
    "¿Cuándo usar prompts básicos?\n",
    "\n",
    "- Para consultas simples donde el contexto no cambia con frecuencia.\n",
    "- Cuando necesites respuestas específicas basadas en instrucciones claras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template_basico = \"\"\"Eres un asistente culinario que responde a preguntas de manera breve.\n",
    "Pregunta: ¿Cuáles son los ingredientes para preparar {plato}?\n",
    "Respuesta:\"\"\"\n",
    "\n",
    "prompt_temp = PromptTemplate(input_variables=[\"plato\"], template=template_basico)\n",
    "promt_value = prompt_temp.format(plato=\"empanadas\")\n",
    "respuesta_openai = llm_openai(promt_value)\n",
    "print(respuesta_openai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ChatPromptTemplates\n",
    "\n",
    "Los ChatPromptTemplates son versiones avanzadas de los prompts, diseñados específicamente para modelos de chat. Agregan roles claros (sistema, humano, modelo) para guiar mejor las interacciones.\n",
    "¿Por qué usar ChatPromptTemplates?\n",
    "\n",
    "- Cuando el modelo de chat necesita contexto adicional o roles claros.\n",
    "- Para construir conversaciones estructuradas y coherentes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo Funciona la Integración\n",
    "\n",
    "- Se crean mensajes individuales con SystemMessagePromptTemplate y HumanMessagePromptTemplate, definiendo el rol y el contenido de cada uno.\n",
    "- Estos mensajes se ensamblan con ChatPromptTemplate para formar un prompt estructurado y apto para modelos de chat.\n",
    "- Finalmente, este prompt combinado se envía al modelo para generar una respuesta contextualizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Definiendo contexto para el sistema\n",
    "prompt_temp_sistema = PromptTemplate(\n",
    "    template=\"Eres un asistente que recomienda alternativas {adjetivo} a productos.\",\n",
    "    input_variables=[\"adjetivo\"],\n",
    ")\n",
    "template_sistema = SystemMessagePromptTemplate(prompt=prompt_temp_sistema)\n",
    "\n",
    "# Prompt para el usuario\n",
    "prompt_temp_humano = PromptTemplate(template=\"{texto}\", input_variables=[\"texto\"])\n",
    "template_humano = HumanMessagePromptTemplate(prompt=prompt_temp_humano)\n",
    "\n",
    "# Creación del chat template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([template_sistema, template_humano])\n",
    "\n",
    "chat_promt_value = chat_prompt.format_prompt(adjetivo=\"económica\", texto=\"iPad\").to_messages()\n",
    "respuesta_chat = chatgpt(chat_promt_value)\n",
    "print(respuesta_chat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Example Selector\n",
    "\n",
    "Los Example Selectors permiten proporcionar ejemplos específicos al modelo para influir en las respuestas. Esto es útil cuando quieres que el modelo siga un formato o estilo específico.\n",
    "¿Cuándo usarlo?\n",
    "\n",
    "- Cuando necesites que el modelo aprenda de ejemplos concretos.\n",
    "- Para preguntas que tienen respuestas definidas y repetitivas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# Lista de ejemplos\n",
    "ejemplos = [\n",
    "    {\"pregunta\": \"¿Cuál es el planeta más cercano al Sol?\", \"respuesta\": \"Mercurio.\"},\n",
    "    {\"pregunta\": \"¿Cuál es el río más largo del mundo?\", \"respuesta\": \"El Amazonas.\"},\n",
    "]\n",
    "\n",
    "promt_temp_ejemplos = PromptTemplate(input_variables=[\"pregunta\", \"respuesta\"], \n",
    "                                     template=\"Pregunta: {pregunta}\\nRespuesta: {respuesta}\")\n",
    "\n",
    "promt_ejemplos = FewShotPromptTemplate(\n",
    "    example_prompt=promt_temp_ejemplos, \n",
    "    examples=ejemplos, \n",
    "    prefix=\"Eres un asistente que responde preguntas breves sobre datos curiosos.\",\n",
    "    suffix=\"Pregunta: {pregunta}\\nRespuesta:\", \n",
    "    input_variables=[\"pregunta\"]\n",
    ")\n",
    "\n",
    "prompt_value = promt_ejemplos.format(pregunta=\"¿Cuál es el animal terrestre más rápido?\")\n",
    "respuesta_ejemplos = llm_openai(prompt_value)\n",
    "print(respuesta_ejemplos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Output Parser\n",
    "\n",
    "Los parsers permiten transformar o estructurar las respuestas del modelo para que sean más útiles en tu aplicación.\n",
    "¿Cuándo usar parsers?\n",
    "- Si necesitas que las respuestas estén en un formato específico (listas, JSON, etc.).\n",
    "- Para integrar la salida del modelo con otras aplicaciones o sistemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser() #elijo el parseador que prefiera\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "template_basico_parser = \"\"\"Cuales son los ingredientes para preparar {plato}\\n{como_parsear}\"\"\"\n",
    "\n",
    "prompt_temp_parser = PromptTemplate(\n",
    "    input_variables=[\"plato\"], \n",
    "    template=template_basico_parser, \n",
    "    partial_variables={\"como_parsear\": format_instructions}\n",
    ")\n",
    "\n",
    "promt_value_parser = prompt_temp_parser.format(plato=\"empanadas\")\n",
    "respuesta_parser = llm_openai(promt_value_parser) #se formula igual que antes y luego se imprime parseada\n",
    "print(output_parser.parse(respuesta_parser))#aca imprime parseada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
